<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://nimafathi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nimafathi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-09T23:30:34+00:00</updated><id>https://nimafathi.github.io/feed.xml</id><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Unifying Autoregressive and Diffusion Models for Better Sequence Generation</title><link href="https://nimafathi.github.io/blog/2024/unifying-ar-diffusion/" rel="alternate" type="text/html" title="Unifying Autoregressive and Diffusion Models for Better Sequence Generation"/><published>2024-04-07T00:00:00+00:00</published><updated>2024-04-07T00:00:00+00:00</updated><id>https://nimafathi.github.io/blog/2024/unifying-ar-diffusion</id><content type="html" xml:base="https://nimafathi.github.io/blog/2024/unifying-ar-diffusion/"><![CDATA[<d-title> <h1>Unifying Autoregressive and Diffusion Models for Better Sequence Generation</h1> <p>Exploring our novel framework that unifies autoregressive and diffusion-based sequence generation through hyperschedules and hybrid noising processes.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#background">Background</a></div> <div><a href="#hyperschedules">Hyperschedules</a></div> <div><a href="#hybrid-noising">Hybrid Noising</a></div> <div><a href="#adaptive-correction">Adaptive Correction</a></div> <div><a href="#results">Results</a></div> <div><a href="#efficiency">Efficiency</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Imagine combining the best of two powerful approaches: <strong>autoregressive (AR) models</strong>—like GPT—which generate sequences efficiently token by token, and <strong>diffusion models</strong>, known for their robustness in iterative refinement. In our recent work presented at the DeLTa Workshop (ICLR 2025), we made this happen. Here's how.</p> <h2 id="background">Background: The Best of Both Worlds</h2> <p>Autoregressive models like GPT are efficient but can't fix their mistakes. Diffusion models can correct themselves iteratively, but at a computational cost. Could we have the efficiency of AR with the robustness of diffusion?</p> <p>We discovered that these seemingly different models actually belong to the same family. AR models can be viewed as an extreme case of diffusion, using a very particular noising schedule.</p> <h2 id="hyperschedules">Introducing Hyperschedules: Bridging AR and Diffusion</h2> <p>To unify AR and diffusion, we introduce <strong>hyperschedules</strong>, unique noise schedules that vary per token position. Traditional diffusion models uniformly apply noise, but our hyperschedules allow each token to have its own noising pattern, smoothly transitioning from AR-like generation to fully iterative diffusion.</p> <figure> <img src="/assets/img/unifying_ar_diff/hyperschedules.png" alt="Hyperschedule Examples"/> <figcaption>Examples of hyperschedules. From left to right: Quenched AR, Flat, Block, Slide Annealing.</figcaption> </figure> <p>These schedules let us generate sequences token by token (like AR) or iteratively (like diffusion)—or anywhere in between.</p> <h2 id="hybrid-noising">Hybrid Noising: Teaching Models to Self-Correct</h2> <p>Diffusion models traditionally use either "absorb" (replacing tokens with a MASK) or "uniform" (randomly changing tokens). We introduced a <strong>hybrid noising process</strong> that combines both methods, allowing our models to detect and correct mistakes during generation.</p> <figure> <img src="/assets/img/unifying_ar_diff/hybrid_noising.png" alt="Hybrid Noising Illustration"/> <figcaption>Hybrid noising introduces small controlled errors (red), guiding the model to learn corrections.</figcaption> </figure> <h2 id="adaptive-correction">Adaptive Correction Sampler: Fixing Mistakes at Inference Time</h2> <p>To fully leverage our hybrid noising, we introduced the <strong>Adaptive Correction Sampler (ACS)</strong>, a novel inference algorithm that lets models revisit and correct previously generated tokens. ACS significantly enhances sequence quality by reducing cumulative errors.</p> <h2 id="results">How Does It Perform? State-of-the-Art Results</h2> <p>We tested our model on several language modeling benchmarks, outperforming existing diffusion-based methods in terms of perplexity and quality-diversity trade-offs.</p> <figure> <table> <thead> <tr> <th>Method</th> <th>PTB</th> <th>WikiText</th> <th>Lambada</th> <th>Pubmed</th> <th>Arxiv</th> </tr> </thead> <tbody> <tr> <td>Transformer (Sahoo et al.)</td> <td>82.1</td><td>25.8</td><td>51.3</td><td>49.0</td><td>41.7</td> </tr> <tr> <td>MDLM (Sahoo et al.)</td> <td>91.0</td><td>33.2</td><td>48.3</td><td>43.1</td><td>37.9</td> </tr> <tr> <td><strong>Hybrid Model (ours)</strong></td> <td><strong>89.9</strong></td><td><strong>30.0</strong></td><td><strong>45.4</strong></td><td><strong>41.2</strong></td><td><strong>37.3</strong></td> </tr> </tbody> </table> <figcaption>Performance comparison across different benchmarks</figcaption> </figure> <figure> <img src="/assets/img/unifying_ar_diff/perplexity_mauve.png" alt="Quality-Diversity Trade-offs"/> <figcaption>Our hybrid models achieve better quality-diversity trade-offs compared to other diffusion models.</figcaption> </figure> <h2 id="efficiency">Efficient Attention Masks for Faster Inference</h2> <p>Our hyperschedules also support KV-caching-compatible attention masks, making our method computationally efficient. This efficiency brings diffusion-based language models closer to the practical performance of AR models, crucial for real-world applications.</p> <h2 id="conclusion">Conclusion: A Unified Future</h2> <p>By unifying autoregressive and diffusion models through hyperschedules, hybrid noising, and adaptive sampling, we open exciting avenues for future research—especially for complex, reasoning-intensive tasks like code generation and logical reasoning.</p> <p>Read the full paper <a href="https://arxiv.org">here</a>.</p> <p>Stay tuned for more exciting updates!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix>]]></content><author><name>Nima Fathi</name></author><summary type="html"><![CDATA[Exploring our novel framework that unifies autoregressive and diffusion-based sequence generation through hyperschedules and hybrid noising processes.]]></summary></entry><entry><title type="html">Exploring Diffusion Language Modeling at ServiceNow</title><link href="https://nimafathi.github.io/blog/2024/diffusion-language-modeling/" rel="alternate" type="text/html" title="Exploring Diffusion Language Modeling at ServiceNow"/><published>2024-04-03T00:00:00+00:00</published><updated>2024-04-03T00:00:00+00:00</updated><id>https://nimafathi.github.io/blog/2024/diffusion-language-modeling</id><content type="html" xml:base="https://nimafathi.github.io/blog/2024/diffusion-language-modeling/"><![CDATA[<d-title> <h1>Exploring Diffusion Language Modeling at ServiceNow</h1> <p>A deep dive into my work on Diffusion Language Modeling and Masked Discrete Diffusions at ServiceNow.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#diffusion-models">Diffusion Models in NLP</a></div> <div><a href="#masked-diffusions">Masked Discrete Diffusions</a></div> <div><a href="#applications">Applications and Impact</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In this post, I'll share insights from my work at ServiceNow in Montreal, where I've been focusing on advancing the field of Diffusion Language Modeling through innovative approaches to Masked Discrete Diffusions.</p> <h2 id="diffusion-models">Diffusion Models in NLP</h2> <p>Diffusion models have revolutionized the field of image generation, but their application to natural language processing presents unique challenges. Unlike images, text is discrete and structured, requiring specialized approaches to diffusion modeling.</p> <h2 id="masked-diffusions">Masked Discrete Diffusions</h2> <p>Masked Discrete Diffusions represent a novel approach to applying diffusion models to text data. This method combines the strengths of traditional diffusion models with the structured nature of language, enabling more effective text generation and manipulation.</p> <h2 id="applications">Applications and Impact</h2> <p>The applications of Diffusion Language Modeling are vast, ranging from text generation to document summarization and beyond. At ServiceNow, we're exploring how these techniques can enhance enterprise applications and improve user experiences.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix>]]></content><author><name>Nima Fathi</name></author><summary type="html"><![CDATA[A deep dive into my work on Diffusion Language Modeling and Masked Discrete Diffusions at ServiceNow.]]></summary></entry><entry><title type="html">Welcome to My Blog</title><link href="https://nimafathi.github.io/blog/2024/welcome-to-my-blog/" rel="alternate" type="text/html" title="Welcome to My Blog"/><published>2024-04-02T00:00:00+00:00</published><updated>2024-04-02T00:00:00+00:00</updated><id>https://nimafathi.github.io/blog/2024/welcome-to-my-blog</id><content type="html" xml:base="https://nimafathi.github.io/blog/2024/welcome-to-my-blog/"><![CDATA[<d-title> <h1>Welcome to My Blog</h1> <p>An introduction to my blog where I'll share thoughts on AI, ML, and research.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#research-interests">Research Interests</a></div> <div><a href="#what-to-expect">What to Expect</a></div> </nav> </d-contents> <p><strong>About Me:</strong> I'm currently pursuing my M.Sc. at McGill University, working under the guidance of Professor Tal Arbel in the Probabilistic Vision Group (PVG). My research focuses on cutting-edge deep-learning techniques, specifically leveraging GANs and diffusion models to advance medical imaging.</p> <h2 id="introduction">Introduction</h2> <p>Welcome to my blog! This is where I'll be sharing my thoughts and experiences in the world of technology and research. I'm particularly excited to share insights from my work at ServiceNow in Montreal, where I've been focusing on Diffusion Language Modeling through Masked Discrete Diffusions.</p> <h2 id="research-interests">Research Interests</h2> <p>My current research interests include:</p> <ul> <li>Generative Adversarial Networks (GANs) in medical imaging</li> <li>Diffusion models for image generation and analysis</li> <li>Diffusion Language Modeling</li> <li>Masked Discrete Diffusions</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="figure"> <img src="/assets/img/medical-ai.jpg" class="figure-img img-fluid rounded" alt="Medical AI"/> <figcaption class="figure-caption">Example of AI in medical imaging</figcaption> </figure> </div> </div> <h2 id="what-to-expect">What to Expect</h2> <p>In this blog, I'll be covering various topics including:</p> <ul> <li>Machine Learning and AI developments</li> <li>Research updates and insights</li> <li>Technical tutorials and guides</li> <li>Industry perspectives</li> <li>Personal projects and experiences</li> </ul> <p>I'm particularly excited to share my work on:</p> <ul> <li>GANs and diffusion models in medical imaging</li> <li>Diffusion Language Modeling</li> <li>And much more!</li> </ul> <p>Stay tuned for more content coming soon!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix>]]></content><author><name>Nima Fathi</name></author><summary type="html"><![CDATA[An introduction to my blog where I'll share thoughts on AI, ML, and research.]]></summary></entry></feed>